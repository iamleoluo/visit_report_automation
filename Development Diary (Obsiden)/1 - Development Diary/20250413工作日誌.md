---
title: 完成將本地llama回應轉換為Python可直接使用的字串
author:
  - 羅暐哲
created: 2025-04-1317:06
location: 
description: 完成將本地llama回應轉換為Python可直接使用的字串
tags:
  - "#ollama"
update files: chat.py
---
### **工作總結：**

今天主要聚焦於本地化的 AI 模型應用，成功架設了 Ollama 並部署 Llama 3.2，在本地環境中充分利用算力資源，達到了節省成本的目的。通過撰寫並測試 Python 介面，不僅完成了將 chatbot 回應轉換為可直接使用的字串，同時也保留了完整的對話歷史，為未來進一步的 prompt engineering 與多應用場景整合打下堅實基礎。此一基礎架構將有助於後續功能擴充與系統優化，之後應該可以不依賴第三方付費 API 。

#### **今日工作內容**

1. **架設 Ollama 與部署 Llama 3.2：**
    
    - 在本機環境架設了 Ollama 伺服器，成功部署 Llama 3.2 模型。
        
    - 以 local 算力運行，節省資源，避免額外成本，未來有望不再需要付費使用 OpenAI。
        
    
2. **Python 介面整合：**
    
    - 完成 Python 程式，將 chatbot 的回應結果生成一個可直接使用的 string，同時保持對話歷史紀錄。
        
    - 實作了對話歷史、系統提示、溫度參數調控以及流式輸出功能。
        
    - 測試結果證明小模型在文本整理上的能力表現良好（見下方附圖）。

#### **程式碼詳細說明**

- **初始化 (Constructor)**
    
    - 利用 ollama.Client 連接到本地 Ollama 服務（預設 URL 為 http://127.0.0.1:11434）。
        
    - 設定模型名稱為 "llama3.2"，並初始化一個空的對話歷史紀錄 conversation_history。
        
- **chat 方法：**
    
    - **輸入與記錄：**
        
        - 接收用戶輸入的文字，並將其以 {"role": "user", "content": user_input} 格式加入歷史紀錄。
            
        - 若提供了 system_prompt，則將其作為首個消息加入，用以調整對話的上下文。
            
        
    - **消息準備：**
        
        - 將目前的歷史紀錄複製到一個新的列表中，並依需插入系統提示。
            
        - 印出發送給 AI 的全部消息內容，便於 debug 及觀察訊息結構。
            
        
    - **API 呼叫與回應處理：**
        
        - 使用 client.chat() 方法向 API 傳送模型名稱、消息列表及其他選項（例如溫度和流式輸出設定）。
            
        - 若為流式輸出，透過遍歷每個回應區塊並累計內容；否則，直接從回應中取出完整的文本。
            
        - 將 AI 回應以 {"role": "assistant", "content": response_text} 格式添加到歷史紀錄中，確保後續對話具有上下文參考。
            
        
    - **錯誤捕捉：**
        
        - 使用 try-except 捕捉可能出現的錯誤，並回傳錯誤訊息，避免程式異常中斷。
            
- **clear_history 方法：**
    
    - 提供一個快速重置對話歷史的方法，使得可以在必要時從頭開始新的對話。
        
    
- **互動式命令行介面：**
    
    - 使用 while 迴圈持續等待用戶輸入。
        
    - 支援 /quit 指令退出、/clear 指令重置對話歷史。
        
    - 每次用戶輸入後，程式會調用 chat() 方法並輸出 AI 的回應。
        
#### **未來規劃與應用展望**

- **基礎架構擴充：**
    
    目前的整合已經能夠將 chatbot 的回應轉換為可直接使用的 string，同時保留完整歷史紀錄，這將成為未來進行 prompt engineering 的基礎模組。
    
- **Prompt Engineering 與功能擴展：**
    
    - 計劃進一步優化系統提示詞，針對不同應用場景進行定制化 prompt 設計。
        
    - 針對特殊應用（例如文本整理、摘要生成等）增加專屬功能模組，讓模型在更複雜的任務中具備更好的表現。
        
    
- **系統效能與本地資源優化：**
    
    - 由於使用本地算力運行模型，不僅降低運行成本，也能更靈活地調整資源分配。
        
    - 探索如何在多任務環境中提高響應速度及處理效能。
        
    
- **成本效益與長期發展：**
    
    - 由於小模型已經證明其文本整理能力穩定且高效，未來可考慮逐步減少對第三方付費 API（如 OpenAI）的依賴，進一步降低成本。
        
    - 根據使用場景的需求，定期更新模型版本與相關配置，確保技術架構具備足夠的彈性與可擴展性。